{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "\n",
    "We have $$Q \\sim \\chi_k^2$$ when $$Q = \\sum_{i=1}^n Z_i^2$$ where $$Z\\sim \\mathcal N(0,1)$$ \n",
    "\n",
    "## Formulation\n",
    "\n",
    "We have our observation $X = {X_1, ...., X_n}$ as our data. \n",
    "Now our assumption is that $X_i \\sim \\mathcal N(\\mu_i, \\sigma_i^2)$. \n",
    "\n",
    "We want a model to learn to predict $\\hat\\mu_i, \\hat\\sigma_i^2$. \n",
    "\n",
    "So, let's consider the RV $Z_i = \\frac{X_i-\\hat \\mu_i}{\\hat\\sigma_i^2}$, according to central limit theorem $Z_i \\sim \\mathcal N(0,1)$. Then we consider a RV $Q = \\sum_{i=1}^K Z_i^2$ where $K$ is a hyperparameter.\n",
    "\n",
    "Then we have that $Q\\sim \\chi^2_K$. And for $K > 50$ $\\chi^2_K \\rightarrow \\mathcal N(K, 2K)$, hence $Q\\sim N(K, 2K)$\n",
    "\n",
    "### Loss Derivation\n",
    "\n",
    "For the loss we use maximum liklihood of RV $Q$ to estimate parameters. So, $$L(\\mathbf{\\hat\\mu}, \\mathbf{\\hat\\sigma^2}) = -\\prod_j^N p(Q_j|K) \\\\ = -\\sum_j^N log p(Q_j|K) = - \\sum_j^N \\frac{-(Q_j-K)^2}{2K} - \\log (2K) \\\\ = \\frac{1}{2K}\\sum_j^N (Q_j-K)^2 \\\\ = \\frac{1}{2K}\\sum_j^N (\\sum_i^K \\frac{(X_i-\\hat\\mu_i)^2}{\\hat\\sigma^2} -K)^2 $$\n",
    "\n",
    "Note that we consider $K$ to be large here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Constant variance, Different means \n",
    "\n",
    "Here we have constant ground truth $\\sigma_i^2 =1 \\space \\forall i$ and different mean $\\mu_i \\forall i \\in {0,...N}$. Then we sample $X_i$ once from each of these $N$ normal distributions. This $X_i$ is our observations and our model will have access to it during training and not the ground truth means and variance. \n",
    "\n",
    "Our aim is to estimate each to these means and variance as $\\hat\\mu_i, \\hat\\sigma_i^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating Training Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  [ 0.5228459   3.79263476 -0.52358763 ... -1.56418854 -3.29082158\n",
      "  4.77050511]\n",
      "Variance: 1\n"
     ]
    }
   ],
   "source": [
    "# N = 5000\n",
    "num_samples = 5000\n",
    "\n",
    "# \\sigma_i^2 = 1\n",
    "gt_var = 1\n",
    "\n",
    "# \\mu_i ~ N(0, 5^2 = 25)\n",
    "gt_mu = np.random.normal(0,5, num_samples)\n",
    "\n",
    "print(\"Mean: \", gt_mu)\n",
    "print(\"Variance:\", gt_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 5000\n"
     ]
    }
   ],
   "source": [
    "gt_observations = np.random.normal(gt_mu, gt_var)\n",
    "gt_observations = torch.tensor(gt_observations)\n",
    "print(\"Training data size:\", len(gt_observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1406, dtype=torch.float64) tensor(26.5118, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(gt_observations.mean(), gt_observations.var())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Procedure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True) tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "pred_mean = torch.ones(num_samples, requires_grad= True)\n",
    "pred_var = torch.ones(num_samples, requires_grad = True)\n",
    "# pred_var = torch.ones(1, requires_grad = True)\n",
    "\n",
    "print(pred_mean, pred_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-2\n",
    "optim = torch.optim.SGD([pred_mean, pred_var], lr = learning_rate)\n",
    "\n",
    "# K = 50\n",
    "deg_freedom = 50\n",
    "\n",
    "batch_size = int(num_samples/deg_freedom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 287/2000 [00:00<00:01, 1366.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:,  tensor(18051.8190, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(2743.3685, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(1979.4288, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(1572.0438, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(1319.0150, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(1141.4607, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(998.6495, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(890.7304, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(805.9882, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(734.6307, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(675.2891, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(623.8623, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(581.2446, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(542.3864, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(507.8245, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(478.5460, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(447.5901, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(422.9736, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(409.6940, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(382.0484, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(364.7905, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(346.4565, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(331.3127, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(316.9524, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(304.9428, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(289.5843, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(282.1667, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(271.2382, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(258.4695, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(247.2293, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(240.7111, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(231.6974, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(224.3479, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(215.6956, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(209.3162, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(203.6799, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(196.2306, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(189.5455, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(185.5886, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(180.6608, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(174.1178, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(170.5403, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(162.1042, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(158.0398, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(155.4330, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(150.1657, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(147.3669, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(143.0102, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(141.6048, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(136.4980, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(132.8296, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(130.2776, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(126.5345, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(123.7438, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(124.2733, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(118.6059, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(117.5562, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(112.6924, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 607/2000 [00:00<00:00, 1476.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(111.8606, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(109.3846, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(106.1370, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(104.2900, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(101.4398, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(100.4618, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(97.3352, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(94.9026, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(92.7657, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(92.6199, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(89.2936, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(88.1000, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(86.3931, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(84.5081, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(84.9739, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(81.9454, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(80.9724, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(77.7403, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(76.4426, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(74.7288, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(74.8376, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(74.0833, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(73.4403, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(70.0339, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(68.8263, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(67.6704, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(67.8616, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(65.7673, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(65.5995, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(63.6784, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(63.1354, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(62.1565, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(60.6346, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(60.6340, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(59.2707, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(59.4960, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(56.6587, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(56.1780, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(56.9249, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(54.5367, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(54.3748, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(53.1248, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(52.9218, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(52.7642, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(52.4820, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(50.1837, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(49.3890, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(48.3697, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(49.2303, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(46.3956, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(46.3041, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(46.9012, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(45.4742, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(44.6450, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(44.5027, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(43.8001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(42.9727, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(41.4249, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(42.3024, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(41.0824, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(41.0902, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(41.3182, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(39.8140, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(38.5424, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 931/2000 [00:00<00:00, 1545.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(39.0948, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(39.0199, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(37.6536, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(37.4278, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(37.1104, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(35.6912, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(36.2532, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(35.1485, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(35.1881, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(35.2001, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(34.0049, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(34.1764, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(33.8522, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(33.0371, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(32.7834, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(33.6318, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(31.8912, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(31.7343, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(30.6002, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(31.0782, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(30.3004, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(29.8855, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(29.3649, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(29.0886, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(29.9358, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(28.2762, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(28.6804, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(27.0840, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(28.1916, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(27.4850, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(26.5068, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(26.9666, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(26.1614, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(26.1135, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(25.9275, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(25.5497, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(25.3956, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(25.8464, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(24.6360, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(24.4739, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(24.9524, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(24.1153, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(23.4120, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(23.8542, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(23.1794, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(23.7563, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(23.1999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(22.4767, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(22.2327, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(21.4038, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(22.0195, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(21.3990, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(21.4884, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(21.4777, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(20.6696, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(20.8113, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(20.6733, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(20.8831, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(20.1049, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(19.6478, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(20.0703, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(19.3914, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(19.6590, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(18.9169, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(19.7711, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(19.4474, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 1252/2000 [00:00<00:00, 1573.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:,  tensor(19.2743, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(18.1655, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(19.2589, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(18.0061, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(19.1551, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(17.7362, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(18.5482, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(17.2855, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(17.4339, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(17.4453, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(17.1597, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(16.2098, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(17.2341, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(16.4041, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(15.8304, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(16.2811, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(16.2638, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(15.6218, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(15.8619, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(15.3465, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(15.1608, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(15.1346, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(15.4256, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(15.2122, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(15.5586, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(14.5906, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(14.7187, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(15.3882, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(14.3116, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(15.3013, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.8719, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(14.6257, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.5281, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(14.0866, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(14.3358, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.6549, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.8985, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(14.2103, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.9372, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.2418, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(12.8281, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.0422, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(12.9525, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.2271, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(12.7688, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(12.0845, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(12.7055, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.3649, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(12.7331, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(12.0965, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.0509, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(12.6683, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(12.3494, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(12.1098, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(12.5707, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.3977, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.6104, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.1832, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.5073, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(12.0835, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.6711, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.2386, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.8663, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.0890, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 1403/2000 [00:00<00:00, 1527.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.5639, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.9382, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.4033, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.5829, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.3020, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.2799, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.6343, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.7718, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.8032, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.1801, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.8937, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.0325, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.5279, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.3543, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.6931, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.2504, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.0962, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.6304, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.2539, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.9786, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.7945, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.3311, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.1666, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.0305, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.8613, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.7085, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.8664, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.1988, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.1793, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.5152, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.0460, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.8422, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.8685, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.5510, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.1038, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.8153, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.3547, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.7252, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.4490, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.4224, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.0786, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.4396, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.4259, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.0817, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.8175, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.1691, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.2987, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.2572, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.9333, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.0118, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.9416, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.0284, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.1737, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.8707, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.4707, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.4155, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.2802, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.2903, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.7074, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 1699/2000 [00:01<00:00, 1464.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:,  tensor(7.1771, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.1287, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.5187, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.2034, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.4244, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.4875, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.4318, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.2759, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.9160, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.9516, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.9917, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.9300, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.7048, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.4135, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.5895, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.7527, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.5346, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.0086, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.5688, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.8140, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.6525, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.3292, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.1534, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.5169, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.9939, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.3080, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.9761, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.0097, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.9942, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.0311, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.2397, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.2124, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.2945, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.0306, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.9392, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.0454, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.1432, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.8488, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.9343, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.5905, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.5891, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.9642, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.9699, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.7501, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.6854, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.4964, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.8669, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.5787, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.3252, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.1575, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.7807, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.5597, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.9369, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.4663, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.1512, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.4572, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.7864, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:01<00:00, 1514.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:,  tensor(5.2843, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.3467, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.6029, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.8361, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.2132, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.9748, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.3637, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.2770, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.8473, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.0640, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.7732, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.7174, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.3424, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.8521, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.9379, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.5572, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.8055, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.9362, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.6175, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.6078, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.5931, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.7203, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.3825, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.7223, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.5132, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.7942, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.8827, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.2895, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.5339, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.5669, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.4198, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(4.2303, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2000\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    index = np.random.permutation(num_samples).reshape((batch_size, deg_freedom))\n",
    "    \n",
    "    chi_stat = ((pred_mean[index] - gt_observations[index])**2/pred_var[index]).sum(axis=1)\n",
    "#     chi_stat = ((pred_mean[index] - gt_observations[index])**2/pred_var).sum(axis=1)\n",
    "    # print(chi_stat.shape)\n",
    "\n",
    "    batch_loss = ((chi_stat- deg_freedom)**2/(2*deg_freedom)).mean()\n",
    "#     batch_loss = ((chi_stat- deg_freedom)**2/(2*deg_freedom)).sum()\n",
    "    # print(batch_loss)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    batch_loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    if epoch%5==0:\n",
    "        print(\"Loss:, \", batch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Mean Error:  0.4296574721733183\n",
      "Absolute Variance Error:  1.9434211\n",
      "Calibration Error:  5.659149291990534\n",
      "tensor([1.0035, 2.7691, 1.6696,  ..., 1.6190, 3.7187, 2.6810],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute Mean Error: \",(pred_mean.detach().numpy()-gt_mu).mean())\n",
    "print(\"Absolute Variance Error: \", (pred_var.detach().numpy()-gt_var).mean())\n",
    "print(\"Calibration Error: \", torch.abs((pred_mean-gt_observations)**2-pred_var).detach().numpy().mean())\n",
    "print(pred_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  0.1893334674902497 Var:  1.2883871296838985\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "normalized_x = ((pred_mean-gt_observations)/torch.sqrt(pred_var)).detach().numpy()\n",
    "print(\"Mean: \", normalized_x.mean(), \"Var: \", normalized_x.var())\n",
    "sns.displot(normalized_x, kind=\"kde\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the variable $Z = \\frac{x-\\mu}{\\sigma}$ really follows $\\mathcal N(0.23,1.27)$. Which is very close to $\\mathcal N(0,1)$, where this minor difference can be due to approximation of $\\chi_K^2 \\approx \\mathcal N(K, 2K)$\n",
    "\n",
    "## Which shows that using chi-sqaure loss works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------xxxxxxxxxxxxxx---------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Different variance, Different means \n",
    "\n",
    "Here we have different ground truth $\\sigma_i \\sim U(1,2)$ and different mean $\\mu_i \\sim \\mathcal N(0,25)$. Then we sample $X_i$ once from each of these $N$ normal distributions. This $X_i$ is our observations and our model will have access to it during training and not the ground truth means and variance. \n",
    "\n",
    "Our aim is to estimate each to these means and variance as $\\hat\\mu_i, \\hat\\sigma_i^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating Training Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  [ 3.64676011  5.18606202 -6.3598712  ... -0.53714615 -6.3228859\n",
      "  0.59822228]\n",
      "Variance: [2.6794792  3.25666612 3.93005144 ... 3.85655768 2.51291954 3.0096082 ]\n"
     ]
    }
   ],
   "source": [
    "# N = 5000\n",
    "num_samples = 5000\n",
    "\n",
    "# \\sigma_i^2 = 1\n",
    "gt_var = np.random.uniform(1,2, num_samples)**2\n",
    "\n",
    "# \\mu_i ~ N(0, 5^2 = 25)\n",
    "gt_mu = np.random.normal(0,5, num_samples)\n",
    "\n",
    "print(\"Mean: \", gt_mu)\n",
    "print(\"Variance:\", gt_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 5000\n"
     ]
    }
   ],
   "source": [
    "gt_observations = np.random.normal(gt_mu, gt_var)\n",
    "gt_observations = torch.tensor(gt_observations)\n",
    "print(\"Training data size:\", len(gt_observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0972, dtype=torch.float64) tensor(31.8513, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(gt_observations.mean(), gt_observations.var())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Procedure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True) tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "pred_mean = torch.ones(num_samples, requires_grad= True)\n",
    "pred_var = torch.ones(num_samples, requires_grad = True)\n",
    "# pred_var = torch.ones(1, requires_grad = True)\n",
    "\n",
    "print(pred_mean, pred_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-2\n",
    "optim = torch.optim.SGD([pred_mean, pred_var], lr = learning_rate)\n",
    "\n",
    "# K = 50\n",
    "deg_freedom = 50\n",
    "\n",
    "batch_size = int(num_samples/deg_freedom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 276/2000 [00:00<00:01, 1292.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:,  tensor(26161.5974, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(3044.5573, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(2244.0592, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(1823.8228, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(1546.1671, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(1333.2079, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(1176.4555, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(1058.9339, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(960.9378, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(872.0375, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(811.7616, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(745.6656, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(694.6853, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(647.3277, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(613.7981, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(574.9563, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(544.4575, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(514.5755, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(494.3322, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(464.2089, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(441.7461, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(422.5378, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(402.6951, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(385.8282, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(369.4597, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(360.6979, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(346.7971, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(328.3657, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(317.8461, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(304.7750, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(298.2483, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(286.6636, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(275.2508, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(267.3341, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(257.8536, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(250.4224, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(240.2513, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(235.4761, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(228.5816, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(220.3181, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(216.8236, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(207.8653, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(202.9138, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(198.4759, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(193.2160, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(188.0629, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(184.0272, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(179.8332, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(172.2999, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(171.1923, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(165.5121, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(162.2417, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(158.5679, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(152.8040, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(151.3689, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(147.7085, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 592/2000 [00:00<00:00, 1423.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:,  tensor(145.0517, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(139.9036, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(139.1275, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(135.2900, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(133.3670, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(129.4272, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(127.5204, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(123.4538, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(124.4710, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(120.5179, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(117.7319, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(116.2633, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(110.7969, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(109.4066, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(108.6190, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(107.4531, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(106.0177, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(102.8593, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(102.0197, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(97.2729, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(97.3092, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(96.1594, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(93.9886, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(92.6456, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(90.5231, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(91.0503, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(87.8255, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(87.9094, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(84.9500, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(85.4124, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(82.5859, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(82.1359, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(79.8829, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(78.7079, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(76.8499, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(76.0549, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(74.5810, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(73.1951, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(72.5419, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(71.5890, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(72.3141, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(70.3540, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(68.5103, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(67.2655, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(67.0152, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(66.4010, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(65.1699, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(63.0564, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(62.4025, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(63.3346, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(60.7699, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(59.1555, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(61.6154, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(59.2042, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(58.6534, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(57.3561, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(56.7220, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(55.7519, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(55.5290, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(55.0033, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(55.9333, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(53.1681, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(52.8169, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 912/2000 [00:00<00:00, 1508.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(51.7771, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(50.1501, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(49.9383, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(50.5074, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(49.5637, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(48.0581, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(47.3962, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(48.7493, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(46.9922, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(45.5746, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(45.1597, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(44.9970, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(45.3132, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(44.7337, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(43.1584, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(43.2969, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(42.5312, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(42.0422, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(42.2317, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(41.0581, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(39.5700, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(41.3816, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(39.5394, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(37.8809, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(38.6443, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(37.8645, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(36.4936, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(37.6008, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(36.7003, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(37.2536, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(35.4969, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(36.4023, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(36.8763, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(35.3838, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(35.5908, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(34.1281, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(34.5049, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(33.7455, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(33.3035, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(32.6850, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(32.3093, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(32.1509, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(31.4233, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(31.5337, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(31.6568, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(31.1690, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(31.0470, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(30.7379, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(30.9449, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(29.7239, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(29.8997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(29.5341, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(29.9058, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(27.9997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(28.8703, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(28.2197, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(28.1173, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(27.7123, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(27.9648, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(27.5369, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(27.4088, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(26.9504, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(26.2878, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(25.7405, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(26.2266, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 1237/2000 [00:00<00:00, 1565.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:,  tensor(25.3678, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(25.1142, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(25.2611, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(25.1231, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(24.9545, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(24.9804, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(24.7708, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(23.8539, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(23.7407, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(23.1653, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(22.6769, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(23.0373, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(23.0033, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(23.7179, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(22.5671, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(22.1293, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(21.4429, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(22.6892, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(21.1614, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(21.9904, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(22.0941, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(21.9191, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(20.9509, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(21.2630, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(21.4245, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(20.5926, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(19.6829, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(19.7378, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(20.5335, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(19.3066, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(19.3806, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(18.5785, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(19.5203, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(19.2182, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(19.4487, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(18.8570, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(19.1860, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(18.1352, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(18.2759, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(18.0344, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(17.7957, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(18.5508, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(18.0651, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(17.8453, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(17.8864, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(17.3942, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(17.9474, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(17.2753, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(17.8748, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(17.4476, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(16.4704, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(17.0855, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(16.9867, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(16.5809, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(16.3799, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(16.1352, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(16.1572, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(16.7887, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(16.5298, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(15.2409, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(14.7604, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(15.6812, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(16.1324, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(15.6907, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(14.9433, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(15.2754, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1551/2000 [00:01<00:00, 1558.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:,  tensor(15.6954, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(14.7864, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(15.3956, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(14.4113, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(15.0440, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(14.4801, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(14.9643, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.6349, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.7751, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.7276, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.9320, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(14.1282, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(12.8759, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.1616, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.4913, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.5692, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.3006, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.3187, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.0650, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(12.8896, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.4444, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.4363, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.2589, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.5609, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.2392, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(13.3036, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(12.4176, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.9853, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(12.4812, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(12.3266, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.9745, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.7979, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.8260, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.5430, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.4379, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(12.2135, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.8307, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.1351, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.6719, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.3322, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.1462, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.9474, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.7499, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.0571, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.7512, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.0876, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.4295, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.4302, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.0481, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.6280, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.9820, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.1037, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(11.0921, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.3507, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.4231, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.8809, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.5660, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.1888, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.1665, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.8619, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.6815, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.5152, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.9512, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 1705/2000 [00:01<00:00, 1489.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:,  tensor(9.4113, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.0642, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.0709, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(10.1125, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.0711, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.1869, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.7414, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.4204, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.0034, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.9528, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.6041, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.1575, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.0180, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(9.5416, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.8437, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.6301, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.8600, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.9489, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.3460, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.4905, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.6040, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.1155, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.1974, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.6751, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.8751, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.3673, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.4005, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.0831, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.1127, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.8080, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.8755, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.2083, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.9535, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.5403, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.5575, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.4233, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.0344, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.2557, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(8.6081, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.4639, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.0209, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.3578, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.1623, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.9733, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.5144, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.0795, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.3475, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.0959, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.4664, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.1253, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.3954, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.9039, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.4654, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.0837, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.0900, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.2495, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.8009, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:01<00:00, 1511.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:,  tensor(7.1265, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.6782, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.5342, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.7936, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.0428, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.0179, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.6734, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.2766, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.2757, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.3073, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(7.5671, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.5072, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.3472, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.9894, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.1214, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.4645, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.2070, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.7706, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.8546, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.2395, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.8407, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.2107, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.9130, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.3718, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.3041, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.7074, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(5.5653, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.4512, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.2426, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Loss:,  tensor(6.2323, dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2000\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    index = np.random.permutation(num_samples).reshape((batch_size, deg_freedom))\n",
    "    \n",
    "    chi_stat = ((pred_mean[index] - gt_observations[index])**2/pred_var[index]).sum(axis=1)\n",
    "#     chi_stat = ((pred_mean[index] - gt_observations[index])**2/pred_var).sum(axis=1)\n",
    "    # print(chi_stat.shape)\n",
    "\n",
    "    batch_loss = ((chi_stat- deg_freedom)**2/(2*deg_freedom)).mean()\n",
    "#     batch_loss = ((chi_stat- deg_freedom)**2/(2*deg_freedom)).sum()\n",
    "    # print(batch_loss)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    batch_loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    if epoch%5==0:\n",
    "        print(\"Loss:, \", batch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Mean Error:  0.45330112175955584\n",
      "Absolute Variance Error:  0.8769886359977384\n",
      "Calibration Error:  6.811064719530656\n",
      "tensor([2.6755, 2.8168, 8.9796,  ..., 1.5852, 5.5392, 1.6911],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute Mean Error: \",(pred_mean.detach().numpy()-gt_mu).mean())\n",
    "print(\"Absolute Variance Error: \", (pred_var.detach().numpy()-gt_var).mean())\n",
    "print(\"Calibration Error: \", torch.abs((pred_mean-gt_observations)**2-pred_var).detach().numpy().mean())\n",
    "print(pred_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  0.1855100584549643 Var:  1.3610111741504927\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "normalized_x = ((pred_mean-gt_observations)/torch.sqrt(pred_var)).detach().numpy()\n",
    "print(\"Mean: \", normalized_x.mean(), \"Var: \", normalized_x.var())\n",
    "sns.displot(normalized_x, kind=\"kde\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the variable $Z = \\frac{x-\\mu}{\\sigma}$ really follows $\\mathcal N(0.18,1.36)$. Which is very close to $\\mathcal N(0,1)$, where this minor difference can be due to approximation of $\\chi_K^2 \\approx \\mathcal N(K, 2K)$\n",
    "\n",
    "## Which shows that using chi-sqaure loss works."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
